{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNAY/A2rZ+0kvffIZW7zRAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhaydon/online-fraud-risk-detection/blob/main/anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Anomaly Detection in Python###\n",
        "The most consistent challenge facing fraud detectionanalysts and data scientists out there is detecting abnormal data trends. The realm of fraud analytics does not escape this reality, albeit with somewhat unique challenges. At the risk of stating the obvious, every risk/fraud team should vigilantly monitor the traffic trends of their online service in order to spot and react to anomalies as soon as possible. \n",
        "\n",
        "There is a seemingly endless set of resources online for visualizing and monitoring traffic trends. In reality, many companies will eschew third-party products and elect instead to utlize a business intelligence (BI) dashboard or tailoring open source Python scripts to analyze and visualize data.\n",
        "\n",
        "A more granular form of considering anomalies is not to look for high-level trends, but rather, to spot abnormal behavior coming from specific users.\n",
        "\n",
        "The following Python code snippets represent a conceptual example of a \"login anamoloy detection\" algorithm. The first piece of code below calculates the daily number of logins per user to build up a statistical history (variable = \"daily_logins_per_user_count\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HanHDRcAAcV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WITH daily_logins_per_user_count AS\n",
        "(\n",
        "  SELECT userID,\n",
        "    LoginDate,\n",
        "    count(*) AS count_logins\n",
        "  FROM user_logins\n",
        "  GROUP BY 1, 2\n",
        ")"
      ],
      "metadata": {
        "id": "RvcrCNrNEkBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After producing the login histogram, the following metrics are recorded in order to measure the level of density/velocity in user logins:\n",
        "\n",
        "• Rate of normal volume days out of total number of active days for this user\n",
        "\n",
        "• Rate of abnormal volume days out of total number of active days for this user\n",
        "\n",
        "• Rate of abnormal volume days out of sum of logins, allowing a focus on users who consistently show abnormal figures versus users who spiked for only a short period of time\n",
        "\n",
        "• Average login rate: simple division of logins out of all active days"
      ],
      "metadata": {
        "id": "85eLbEaFE-fc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Q0g2lEAKAn"
      },
      "outputs": [],
      "source": [
        "SELECT userID,\n",
        "sum(CASE\n",
        "    WHEN count_logins = 1 THEN 1\n",
        "    ELSE 0\n",
        "  END)/count(*) AS perc_of_1_logins_in_perc,\n",
        "sum(CASE\n",
        "    WHEN count_logins >1 THEN 1\n",
        "    ELSE 0 END)/count(*) AS perc_of_above_1_logins_in_perc,\n",
        "sum(CASE\n",
        "    WHEN count_logins >1 THEN count_logins\n",
        "    ELSE 0\n",
        "  END)/sum(count_logins) AS perc_of_above_1_logins_sum(count_logins)\n",
        "sum(count_logins) total_login\n",
        "  FROM daily_logins_per_user_count\n",
        "GROUP BY 1;"
      ]
    }
  ]
}
